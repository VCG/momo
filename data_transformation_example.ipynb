{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import navis\n",
    "from compcon.create_graph import get_neuron_local\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import arkouda as ak\n",
    "sys.path.insert(0, os.path.abspath(\"/home/michaelshewarega/Desktop/test/arkouda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code preprocesses the synapses and maps them to the correct segments. \n",
    "# Since in our new data structure, neuron segments are represented as nodes, \n",
    "# synapses must be mapped to the appropriate segments of each neuron.\n",
    "# \n",
    "# To achieve this, we create a dictionary called `global_mapping`, which stores \n",
    "# neuron IDs as keys. For each neuron ID, the dictionary stores the corresponding \n",
    "# synapse index and the neuron segment it maps to.\n",
    "\n",
    "import random\n",
    "#loading the synapses and the pre and postsynaptic ids of the neurons for each synapse\n",
    "syn_df = pd.read_csv(\"path_to/synapses.csv\")\n",
    "pre = syn_df['pre_pt_root_id'].drop_duplicates().values.tolist()\n",
    "post = syn_df['post_pt_root_id'].drop_duplicates().values.tolist()\n",
    "syn_ids = list(set(pre + post))\n",
    "gloabl_mapping = {}\n",
    "\n",
    "for n_id in ids:\n",
    "    # Create a consistent random multiplier based on the neuron id\n",
    "    random.seed(int(n_id), 1000)\n",
    "    consistent_multiplier = random.randint(1, 1000000000000)\n",
    "    \n",
    "    x_temp=[]\n",
    "    y_temp=[]\n",
    "    z_temp=[]\n",
    "    #status of progress\n",
    "    print((syn_ids.index(n_id)/len(syn_ids))*100, \"%\")\n",
    "\n",
    "    #filtered synpases with either pre or post synaptic id being the one of the neuron\n",
    "    filtered_df = syn_df[(syn_df['pre_pt_root_id'] == n_id) | (syn_df['post_pt_root_id'] == n_id)]\n",
    "\n",
    "    #indexes of the synapses\n",
    "    indexes = filtered_df.index.tolist()\n",
    "\n",
    "    #load current neuron\n",
    "    cur_neuron = get_neuron_local(n_id)\n",
    "\n",
    "    #get x, y, z coordinates of neurons\n",
    "    x_temp.extend((filtered_df.loc[:, \"pre_pt_position_x\"].values/1000 + filtered_df.loc[:, \"post_pt_position_x\"].values/1000)/2)\n",
    "    y_temp.extend((filtered_df.loc[:, \"pre_pt_position_y\"].values/1000 + filtered_df.loc[:, \"post_pt_position_y\"].values/1000)/2)\n",
    "    z_temp.extend((filtered_df.loc[:, \"pre_pt_position_z\"].values/1000 + filtered_df.loc[:, \"post_pt_position_z\"].values/1000)/2)\n",
    "    \n",
    "    c_coordinates = np.vstack((x_temp, y_temp, z_temp)).T\n",
    "\n",
    "    #snap synapses to skeleton in neuron\n",
    "    idloc1, dist1 = cur_neuron.snap(c_coordinates)\n",
    "\n",
    "    #downsample neuron\n",
    "    cur_neuron_ds = navis.downsample_neuron(cur_neuron, downsampling_factor=10000, inplace=False)\n",
    "\n",
    "    #downsample neuron and preserve the skeletons to which the synapses where snapped to\n",
    "    cur_neuron_ds_preserved = navis.downsample_neuron(cur_neuron, downsampling_factor=10000, preserve_nodes=idloc1, inplace=False)\n",
    "\n",
    "    #map0 are the skeletons that will map to the neuron segments stored in map1\n",
    "    map0 = []\n",
    "    map1 = []\n",
    "    cur_neuron_ds_array = np.array(cur_neuron_ds.edges)\n",
    "    \n",
    "    map0 = map0 + list(cur_neuron_ds_array[:,0])\n",
    "    map1 = map1 + list(cur_neuron_ds_array[:,0]*consistent_multiplier + cur_neuron_ds_array[:,1])\n",
    "\n",
    "    map0 = map0 + list(cur_neuron_ds_array[:,1])\n",
    "    map1 = map1 + list(cur_neuron_ds_array[:,0]*consistent_multiplier + cur_neuron_ds_array[:,1])\n",
    "\n",
    "    cur_neuron_dsedges=cur_neuron_ds.edges\n",
    "    cur_neuron_ds = set(map(tuple, cur_neuron_ds.edges))\n",
    "    cur_neuron_ds_preserved = set(map(tuple, cur_neuron_ds_preserved.edges))\n",
    "    \n",
    "    # Find the difference\n",
    "    difference_set1 = cur_neuron_ds - cur_neuron_ds_preserved\n",
    "    difference_set2 = cur_neuron_ds_preserved - cur_neuron_ds\n",
    "\n",
    "    difference_set1 = list(map(list, difference_set1))\n",
    "    difference_set2 = list(map(list, difference_set2))\n",
    "\n",
    "    nx_display = nx.Graph()\n",
    "    nx_display.add_edges_from(difference_set2)\n",
    "    \n",
    "    for eds in difference_set1:\n",
    "        start_node_id = eds[0]\n",
    "        end_node_id = eds[1]\n",
    "        \n",
    "        shortest_path = nx.shortest_path(nx_display, source=start_node_id, target=end_node_id, weight='weight')\n",
    "\n",
    "        if len(shortest_path) > 2:\n",
    "            inner_path = shortest_path[1:-1]\n",
    "            map0.extend(inner_path)\n",
    "            map1.extend(len(inner_path)*[eds[0]*consistent_multiplier+eds[1]])\n",
    "\n",
    "    \n",
    "    mapping0 = dict(zip(map0, map1))\n",
    "    idloc1 = np.vectorize(mapping0.get)(idloc1).tolist()\n",
    "\n",
    "    index_to_id_mapping = dict(zip(indexes, idloc1))\n",
    "    gloabl_mapping[n_id] = index_to_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the neurons as input and generates all possible combinations \n",
    "# of them. It then maps the corresponding synapses to the appropriate neuron segments \n",
    "# using the 'global_mapping' dictionary. Finally, the function returns the synapse \n",
    "# information for each combination.\n",
    "def synaptic_data(neurons):\n",
    "    synaptic_src, synaptic_s_bef, synaptic_s_af, synaptic_d_bef, synaptic_d_af, synaptic_dst, x, y, z, n_combo, dists = [], [], [], [], [], [], [], [], [], [], []\n",
    "    filtered_df = pd.read_csv(\"/path_to/synapses.csv\")\n",
    "    combinations = filtered_df[['pre_pt_root_id', 'post_pt_root_id']].drop_duplicates().values.tolist()\n",
    "\n",
    "    for combo in combinations:\n",
    "            \n",
    "            x_temp, y_temp, z_temp = [], [], []\n",
    "            \n",
    "            df1 = filtered_df[(filtered_df['pre_pt_root_id'] == combo[0]) & (filtered_df['post_pt_root_id'] == combo[1])]\n",
    "            \n",
    "            if df1.empty:\n",
    "                continue\n",
    "                \n",
    "            print(combo, (combinations.index(combo)/len(combinations))*100, \"%\")\n",
    "            indexes = df1.index.tolist()\n",
    "\n",
    "            x_temp.extend((df1.loc[:, \"pre_pt_position_x\"].values/1000 + df1.loc[:, \"post_pt_position_x\"].values/1000)/2)\n",
    "            y_temp.extend((df1.loc[:, \"pre_pt_position_y\"].values/1000 + df1.loc[:, \"post_pt_position_y\"].values/1000)/2)\n",
    "            z_temp.extend((df1.loc[:, \"pre_pt_position_z\"].values/1000 + df1.loc[:, \"post_pt_position_z\"].values/1000)/2)\n",
    "\n",
    "            mapping0 = gloabl_mapping[combo[0]]\n",
    "            mapping1 = gloabl_mapping[combo[1]]\n",
    "            \n",
    "            idloc1 = np.vectorize(mapping0.get)(indexes).tolist()\n",
    "            idloc2 = np.vectorize(mapping1.get)(indexes).tolist()\n",
    "            \n",
    "            synaptic_s_bef_temp = np.vectorize(lambda x: neurons[combo[0]][\"bef_mapping\"].get(x, 0))(idloc1)\n",
    "            synaptic_s_af_temp = np.vectorize(lambda x: neurons[combo[0]][\"af_mapping\"].get(x, 0))(idloc1)\n",
    "            synaptic_d_bef_temp = np.vectorize(lambda x: neurons[combo[1]][\"bef_mapping\"].get(x, 0))(idloc2)\n",
    "            synaptic_d_af_temp = np.vectorize(lambda x: neurons[combo[1]][\"af_mapping\"].get(x, 0))(idloc2)\n",
    "            \n",
    "            synaptic_src.extend(idloc1)\n",
    "            synaptic_s_bef.extend(synaptic_s_bef_temp)\n",
    "            synaptic_s_af.extend(synaptic_s_af_temp)\n",
    "            synaptic_dst.extend(idloc2)\n",
    "            synaptic_d_bef.extend(synaptic_d_bef_temp)\n",
    "            synaptic_d_af.extend(synaptic_d_af_temp)\n",
    "            n_combo.extend([(f\"{combo[0]}_{combo[1]}\")] * len(idloc1))\n",
    "            x.extend(x_temp)\n",
    "            y.extend(y_temp)\n",
    "            z.extend(z_temp)\n",
    "            dists.extend([0]* len(idloc1))\n",
    "            \n",
    "    connection_type_s = [\"s\"] * len(synaptic_src)\n",
    "    \n",
    "    return synaptic_src, synaptic_dst, connection_type_s, x, y, z, n_combo, dists, synaptic_s_bef, synaptic_s_af, synaptic_d_bef, synaptic_d_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the edges of a downsampled neuron and the downsampled neuron itself, \n",
    "# and constructs a neuron morphology-aware graph, excluding the synapses from the graph creation.\n",
    "def spatial_connectome_creation(edges, n_ds):\n",
    "    # Create a consistent random multiplier based on the neuron ID\n",
    "    random.seed(int(n_ds.name))\n",
    "    consistent_multiplier = random.randint(1, 1000000000000)\n",
    "    \n",
    "    ### create new Graph ### \n",
    "    first_elements = edges[:, 0]\n",
    "    second_elements = edges[:, 1]\n",
    "    neighbor_matrix = (first_elements[:, np.newaxis] == second_elements[np.newaxis, :])\n",
    "\n",
    "    # Step 3: Extract neighbor pairs\n",
    "    neighbor_indices = np.argwhere(neighbor_matrix)\n",
    "    neighbor_indices = neighbor_indices[neighbor_indices[:, 0] != neighbor_indices[:, 1]]\n",
    "    neighbor_pairs = edges[neighbor_indices]\n",
    "\n",
    "    # Step 4: Combine node indices to create unique identifiers for edges\n",
    "    src_combined = neighbor_pairs[:, 0, 0] * consistent_multiplier + neighbor_pairs[:, 0, 1]\n",
    "    dst_combined = neighbor_pairs[:, 1, 0] * consistent_multiplier + neighbor_pairs[:, 1, 1]\n",
    "    \n",
    "    # Convert to lists for Arkouda\n",
    "    src = src_combined.astype(np.int64).tolist()\n",
    "    dst = dst_combined.astype(np.int64).tolist()\n",
    "\n",
    "    ### get coordinates and calculate distances of nodes ###\n",
    "    s_bef = neighbor_pairs[:, 0, 0].astype(np.int64).tolist()\n",
    "    s_af = neighbor_pairs[:, 0, 1].astype(np.int64).tolist()\n",
    "\n",
    "    d_bef = neighbor_pairs[:, 1, 0].astype(np.int64).tolist()\n",
    "    d_af = neighbor_pairs[:, 1, 1].astype(np.int64).tolist()\n",
    "\n",
    "    #mapping\n",
    "    bef_mapping = dict(zip(src + dst, s_bef + d_bef))\n",
    "    af_mapping = dict(zip(src + dst, s_af + d_af))\n",
    "\n",
    "    # Create DataFrame for s_bef and s_af with their corresponding coordinates\n",
    "    s_bef_coords = n_ds.nodes.set_index('node_id').loc[s_bef][['x', 'y', 'z']].reset_index()\n",
    "    s_af_coords = n_ds.nodes.set_index('node_id').loc[s_af][['x', 'y', 'z']].reset_index()\n",
    "\n",
    "    d_bef_coords = n_ds.nodes.set_index('node_id').loc[d_bef][['x', 'y', 'z']].reset_index()\n",
    "    d_af_coords = n_ds.nodes.set_index('node_id').loc[d_af][['x', 'y', 'z']].reset_index()\n",
    "\n",
    "    # Ensure lengths match by repeating coordinates for each occurrence\n",
    "    s_bef_coords_repeated = s_bef_coords.loc[s_bef_coords.index.repeat(s_bef_coords.index.value_counts().reindex(s_bef_coords.index).fillna(1))]\n",
    "    s_af_coords_repeated = s_af_coords.loc[s_af_coords.index.repeat(s_af_coords.index.value_counts().reindex(s_af_coords.index).fillna(1))]\n",
    "\n",
    "    d_bef_coords_repeated = d_bef_coords.loc[d_bef_coords.index.repeat(d_bef_coords.index.value_counts().reindex(d_bef_coords.index).fillna(1))]\n",
    "    d_af_coords_repeated = d_af_coords.loc[d_af_coords.index.repeat(d_af_coords.index.value_counts().reindex(d_af_coords.index).fillna(1))]\n",
    "\n",
    "    # Extract coordinates as numpy arrays\n",
    "    x_bef, y_bef, z_bef = s_bef_coords_repeated['x'].values, s_bef_coords_repeated['y'].values, s_bef_coords_repeated['z'].values\n",
    "    x_af, y_af, z_af = s_af_coords_repeated['x'].values, s_af_coords_repeated['y'].values, s_af_coords_repeated['z'].values\n",
    "\n",
    "    s_bef_x = x_bef.astype(np.int64).tolist()\n",
    "    s_bef_y = y_bef.astype(np.int64).tolist()\n",
    "    s_bef_z = z_bef.astype(np.int64).tolist()\n",
    "    \n",
    "    s_af_x = x_af.astype(np.int64).tolist()\n",
    "    s_af_y = y_af.astype(np.int64).tolist()\n",
    "    s_af_z = z_af.astype(np.int64).tolist()\n",
    "    \n",
    "    s_distances = np.sqrt((x_bef - x_af)**2 + (y_bef - y_af)**2 + (z_bef - z_af)**2).tolist()\n",
    "    s_x = ((x_bef + x_af)/2).tolist()\n",
    "    s_y = ((y_bef + y_af)/2).tolist()\n",
    "    s_z = ((z_bef + z_af)/2).tolist()\n",
    "\n",
    "    ratios = np.linspace(0.1, 0.9, num=3)  # 9 evenly spaced points between 0.1 and 0.9\n",
    "\n",
    "    # Generate sampling points for each coordinate\n",
    "    s_x_t = [r * x_b + (1 - r) * x_a for r in ratios for x_b, x_a in zip(x_bef, x_af)]\n",
    "    s_y_t = [r * y_b + (1 - r) * y_a for r in ratios for y_b, y_a in zip(y_bef, y_af)]\n",
    "    s_z_t = [r * z_b + (1 - r) * z_a for r in ratios for z_b, z_a in zip(z_bef, z_af)]\n",
    "\n",
    "    x_bef, y_bef, z_bef = d_bef_coords_repeated['x'].values, d_bef_coords_repeated['y'].values, d_bef_coords_repeated['z'].values\n",
    "    x_af, y_af, z_af = d_af_coords_repeated['x'].values, d_af_coords_repeated['y'].values, d_af_coords_repeated['z'].values\n",
    "\n",
    "    d_bef_x = x_bef.astype(np.int64).tolist()\n",
    "    d_bef_y = y_bef.astype(np.int64).tolist()\n",
    "    d_bef_z = z_bef.astype(np.int64).tolist()\n",
    "\n",
    "    d_af_x = x_af.astype(np.int64).tolist()\n",
    "    d_af_y = y_af.astype(np.int64).tolist()\n",
    "    d_af_z = z_af.astype(np.int64).tolist()\n",
    "\n",
    "    # Calculate distances using vectorized operations\n",
    "    d_distances = np.sqrt((x_bef - x_af)**2 + (y_bef - y_af)**2 + (z_bef - z_af)**2).tolist()\n",
    "    d_x = ((x_bef + x_af)/2).tolist()\n",
    "    d_y = ((y_bef + y_af)/2).tolist()\n",
    "    d_z = ((z_bef + z_af)/2).tolist()\n",
    "\n",
    "    # Generate sampling points for each coordinate\n",
    "    d_x_t = [r * x_b + (1 - r) * x_a for r in ratios for x_b, x_a in zip(x_bef, x_af)]\n",
    "    d_y_t = [r * y_b + (1 - r) * y_a for r in ratios for y_b, y_a in zip(y_bef, y_af)]\n",
    "    d_z_t = [r * z_b + (1 - r) * z_a for r in ratios for z_b, z_a in zip(z_bef, z_af)]\n",
    "\n",
    "    connection_type_n = [\"n\"] * len(src)\n",
    "    \n",
    "    return src, dst, s_bef, s_bef_x, s_bef_y, s_bef_z, s_af, s_af_x, s_af_y, s_af_z, s_x, s_y, s_z, s_x_t, s_y_t, s_z_t, s_distances, d_bef, d_bef_x, d_bef_y, d_bef_z, d_af, d_af_x, d_af_y, d_af_z, d_x, d_y, d_z, d_x_t, d_y_t, d_z_t , d_distances, connection_type_n, bef_mapping, af_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code combines the previously defined functions and, in the end, creates and returns \n",
    "# the dictionary representing the neuron morphology aware graph.\n",
    "def create_neuron_morphology_aware_graph(id_list):\n",
    "    #ensure no duplicate ids\n",
    "    id_list= list(id_list)\n",
    "\n",
    "    #to remove neurons that failed loading\n",
    "    id_list_copy= id_list.copy()\n",
    "\n",
    "    #all of the columns in the new df\n",
    "    neurons={}\n",
    "    src, dst, s_bef, s_bef_x, s_bef_y, s_bef_z, s_af, s_af_x, s_af_y, s_af_z, s_x, s_y, s_z, s_x_2, s_y_2, s_z_2, s_x_3, s_y_3, s_z_3, s_x_t, s_y_t, s_z_t, s_distances, d_bef, d_bef_x, d_bef_y, d_bef_z, d_af, d_af_x, d_af_y, d_af_z, d_x, d_y, d_z, d_x_2, d_y_2, d_z_2, d_x_3, d_y_3, d_z_3, d_x_t, d_y_t, d_z_t, d_distances, connection_type_n, n_id = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "    bef_mapping, af_mapping = {}, {}\n",
    "    for id in id_list:\n",
    "        #load neuron\n",
    "        n= get_neuron_local(id, None, 100000)\n",
    "\n",
    "        if n == None:\n",
    "             id_list_copy.remove(id)\n",
    "             continue\n",
    "        \n",
    "        neurons[id] = {\"n\": n}\n",
    "        edges = n.edges\n",
    "        #create morphology aware graph without synapses for each neuron\n",
    "        src_temp, dst_temp, s_bef_temp, s_bef_x_temp, s_bef_y_temp, s_bef_z_temp, s_af_temp, s_af_x_temp, s_af_y_temp, s_af_z_temp, s_x_temp, s_y_temp, s_z_temp, s_x_t_temp, s_y_t_temp, s_z_t_temp, s_distances_temp, d_bef_temp, d_bef_x_temp, d_bef_y_temp, d_bef_z_temp, d_af_temp, d_af_x_temp, d_af_y_temp, d_af_z_temp, d_x_temp, d_y_temp, d_z_temp, d_x_t_temp, d_y_t_temp, d_z_t_temp, d_distances_temp, connection_type_n_temp, bef_mapping, af_mapping = spatial_connectome_creation(edges, n)\n",
    "        \n",
    "        neurons[id].update({\"src\": src_temp, \"dst\": dst_temp, \"s_x\": s_x_temp, \"s_y\": s_y_temp, \"s_z\": s_z_temp, \"s_x\": s_x_temp, \"s_y\": s_y_temp, \"s_z\": s_z_temp, \"s_x_t\": s_x_t_temp, \"s_y_t\": s_y_t_temp, \"s_z_t\": s_z_t_temp , \"d_x\": d_x_temp, \"d_y\": d_y_temp, \"d_z\": d_z_temp, \"d_x_t\": d_x_t_temp, \"d_y_t\": d_y_t_temp, \"d_z_t\": s_z_t_temp , \"bef_mapping\": bef_mapping, \"af_mapping\": af_mapping})\n",
    "        \n",
    "        src.extend(src_temp)\n",
    "        dst.extend(dst_temp)\n",
    "        s_bef.extend(s_bef_temp)\n",
    "        s_bef_x.extend(s_bef_x_temp)\n",
    "        s_bef_y.extend(s_bef_y_temp)\n",
    "        s_bef_z.extend(s_bef_z_temp)\n",
    "        s_af.extend(s_af_temp)\n",
    "        s_af_x.extend(s_af_x_temp)\n",
    "        s_af_y.extend(s_af_y_temp)\n",
    "        s_af_z.extend(s_af_z_temp)\n",
    "        s_x.extend(s_x_temp)\n",
    "        s_y.extend(s_y_temp)\n",
    "        s_z.extend(s_z_temp)\n",
    "        s_distances.extend(s_distances_temp)\n",
    "        d_bef.extend(d_bef_temp)\n",
    "        d_bef_x.extend(d_bef_x_temp)\n",
    "        d_bef_y.extend(d_bef_y_temp)\n",
    "        d_bef_z.extend(d_bef_z_temp)\n",
    "        d_af.extend(d_af_temp)\n",
    "        d_af_x.extend(d_af_x_temp)\n",
    "        d_af_y.extend(d_af_y_temp)\n",
    "        d_af_z.extend(d_af_z_temp)\n",
    "        d_x.extend(d_x_temp)\n",
    "        d_y.extend(d_y_temp)\n",
    "        d_z.extend(d_z_temp)\n",
    "        d_distances.extend(d_distances_temp)\n",
    "        connection_type_n.extend(connection_type_n_temp)\n",
    "        n_id.extend([id]* len(src_temp))\n",
    "    \n",
    "    if id_list_copy == []:\n",
    "        return None\n",
    "    \n",
    "    #include the synapses to the neuron morphology aware graph\n",
    "    synaptic_src, synaptic_dst, connection_type_s, x, y, z, n_combo, dists, synaptic_s_bef, synaptic_s_af, synaptic_d_bef, synaptic_d_af= synaptic_data(neurons)\n",
    "\n",
    "    spatial_connectome_edge_dict = {\n",
    "    \"src\": src + dst + synaptic_src, #id of source segment\n",
    "    \"dst\": dst + src+  synaptic_dst, #id of destination segment\n",
    "    \"s_bef\": s_bef + d_bef+ synaptic_s_bef, #id of starting skeleton of source segment\n",
    "    \"s_bef_x\": s_bef_x + d_bef_x+ [0]* len(synaptic_src), #x coordinate of starting skeleton\n",
    "    \"s_bef_y\": s_bef_y +d_bef_y+ [0]* len(synaptic_src), #y coordinate of starting skeleton\n",
    "    \"s_bef_z\": s_bef_z + d_bef_z+ [0]* len(synaptic_src), #z coordinate of starting skeleton\n",
    "    \"s_af\": s_af+ d_af+ synaptic_s_af, #id of ending skeleton of source segment\n",
    "    \"s_af_x\": s_af_x+d_af_x+ [0]* len(synaptic_src), #x coordinate of ending skeleton\n",
    "    \"s_af_y\": s_af_y+d_af_y+ [0]* len(synaptic_src), #y coordinate of ending skeleton\n",
    "    \"s_af_z\": s_af_z+d_af_z+ [0]* len(synaptic_src), #z coordinate of ending skeleton\n",
    "    \"s_x\": s_x+d_x+ x, #middle point of source segment\n",
    "    \"s_y\": s_y+d_y+ y, #middle point of source segment\n",
    "    \"s_z\": s_z+d_z+ z, #middle point of source segment\n",
    "    \"s_distance\": s_distances+d_distances+ dists, #length of source segment\n",
    "    \"d_bef\": d_bef+s_bef+ synaptic_d_bef, #id of starting skeleton of destination segment\n",
    "    \"d_bef_x\": d_bef_x+s_bef_x+ [0]* len(synaptic_src), #x coordinate of starting skeleton\n",
    "    \"d_bef_y\": d_bef_y+s_bef_y+ [0]* len(synaptic_src), #y coordinate of starting skeleton\n",
    "    \"d_bef_z\": d_bef_z+s_bef_z+ [0]* len(synaptic_src), #z coordinate of starting skeleton\n",
    "    \"d_af\": d_af+s_af+ synaptic_d_af, #id of ending skeleton of destination segment\n",
    "    \"d_af_x\": d_af_x+s_af_x+ [0]* len(synaptic_src), #x coordinate of ending skeleton\n",
    "    \"d_af_y\": d_af_y+s_af_y+ [0]* len(synaptic_src), #y coordinate of ending skeleton\n",
    "    \"d_af_z\": d_af_z+s_af_z+ [0]* len(synaptic_src), #z coordinate of ending skeleton\n",
    "    \"d_x\": d_x+s_x+ x, #middle point of destination segment\n",
    "    \"d_y\": d_y+s_y+ y, #middle point of destination segment\n",
    "    \"d_z\": d_z+s_z+ z, #middle point of destination segment\n",
    "    \"d_distance\": d_distances+s_distances+ dists, #length of destination segment\n",
    "    \"n_id\": n_id +n_id+ n_combo, #id (two ids if synaptic connection) of neuron\n",
    "    \"connection_type\": connection_type_n +connection_type_n+ connection_type_s #specifies wether intra or inter neuron connection\n",
    "    }\n",
    "\n",
    "    return spatial_connectome_edge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code on how to create and store the neuron morphology aware graph given a list of neuron ids\n",
    "neuron_ids=None #provide neuron ids list\n",
    "spatial_connectome_edge_dict = create_neuron_morphology_aware_graph(neuron_ids)\n",
    "spatial_connectome_edge_dict.to_csv(\"/path_to/spatial_connectome_edge_df.csv\", index=False)\n",
    "spatial_connectome_edge_df = pd.DataFrame(spatial_connectome_edge_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
